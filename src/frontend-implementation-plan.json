{
  "kind": "implementation_plan",
  "version": "1.0",
  "title": "Add SEO-friendly robots.txt allowing Googlebot, Bingbot, and PerplexityBot on all paths",
  "requirements": [
    {
      "id": "REQ-37",
      "summary": "Create a public robots.txt that explicitly allows Googlebot, Bingbot, and PerplexityBot to crawl all paths with no disallow rules.",
      "acceptanceCriteria": [
        "A new file exists at `frontend/public/robots.txt`.",
        "`robots.txt` contains explicit rules for `User-agent: Googlebot`, `User-agent: Bingbot`, and `User-agent: PerplexityBot` that allow crawling of `/` (e.g., `Allow: /`).",
        "There are no `Disallow:` rules that block any paths.",
        "The file is served publicly at `https://<deployed-domain>/robots.txt` in the deployed frontend canister."
      ],
      "file_operations": [
        {
          "path": "frontend/public/robots.txt",
          "operation": "create",
          "description": "Create a standard SEO-friendly robots.txt with three explicit user-agent blocks (Googlebot, Bingbot, PerplexityBot), each containing `Allow: /`, and no `Disallow:` rules."
        }
      ]
    },
    {
      "id": "REQ-38",
      "summary": "Ensure the exact final robots.txt contents can be provided verbatim in completion notes.",
      "acceptanceCriteria": [
        "Completion notes include the full `robots.txt` contents exactly as committed (verbatim, including line breaks)."
      ],
      "file_operations": [
        {
          "path": "frontend/public/robots.txt",
          "operation": "modify",
          "description": "Finalize and verify the committed robots.txt content is exactly what will be pasted verbatim into completion notes (including line breaks), matching the explicit allow rules and containing no disallow rules."
        }
      ]
    }
  ]
}